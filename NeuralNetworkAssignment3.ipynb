{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkAssignment3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/msk180001/NueralNetwork/blob/master/NeuralNetworkAssignment3.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Sy1X5RAO4qeb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_yYapqa4rak",
        "colab_type": "code",
        "outputId": "5678f645-38bd-4f9b-b310-a756aef97861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self, train, header = True, h1 = 4, h2 = 2):\n",
        "        np.random.seed(1)\n",
        "        # train refers to the training dataset\n",
        "        # test refers to the testing dataset\n",
        "        # h1 and h2 represent the number of nodes in 1st and 2nd hidden layers\n",
        "\n",
        "        raw_input = pd.read_csv(train, sep = ',', header=None)\n",
        "        #  Remember to implement the preprocess method\n",
        "        train_dataset = self.preprocess(raw_input)\n",
        "        ncols = len(train_dataset.columns)\n",
        "        nrows = len(train_dataset.index)\n",
        "        self.X = train_dataset.iloc[:, 0:(ncols - 1)].values.reshape(nrows, ncols - 1)\n",
        "        self.y = train_dataset.iloc[:, (ncols - 1)].values.reshape(nrows, 1)\n",
        "        #splitting the data into training and test set\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X,self.y)\n",
        "        #\n",
        "        # Find number of input and output layers from the dataset\n",
        "        #\n",
        "        input_layer_size = len(self.X_train[0])\n",
        "        if not isinstance(self.y_train[0], np.ndarray):\n",
        "            output_layer_size = 1\n",
        "        else:\n",
        "            output_layer_size = len(self.y_train[0])\n",
        "\n",
        "        # assign random weights to matrices in network\n",
        "        # number of weights connecting layers = (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
        "        self.w01 = 2 * np.random.random((input_layer_size, h1)) - 1\n",
        "        self.X01 = self.X_train\n",
        "        self.delta01 = np.zeros((input_layer_size, h1))\n",
        "        self.w12 = 2 * np.random.random((h1, h2)) - 1\n",
        "        self.X12 = np.zeros((len(self.X_train), h1))\n",
        "\n",
        "        self.delta12 = np.zeros((h1, h2))\n",
        "        self.w23 = 2 * np.random.random((h2, output_layer_size)) - 1\n",
        "        self.X23 = np.zeros((len(self.X_train), h2))\n",
        "        self.delta23 = np.zeros((h2, output_layer_size))\n",
        "        self.deltaOut = np.zeros((output_layer_size, 1))\n",
        "    # sigmoid activation function\n",
        "    # tanh activation function\n",
        "    # ReLu activation function\n",
        "\n",
        "    def __activation(self, x, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            return self.__sigmoid(x)\n",
        "        elif activation == \"tanh\":\n",
        "            return self.__tanh(x)\n",
        "        else:\n",
        "            return self.__Relu(x)\n",
        "\n",
        "\n",
        "    #\n",
        "    # Derivative for tanh, ReLu and sigmoid activation function\n",
        "    #\n",
        "\n",
        "    def __activation_derivative(self, x, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            self.__sigmoid_derivative(x)\n",
        "        elif activation == \"tanh\":\n",
        "            self.__tanh_derivative(x)\n",
        "\n",
        "        else:\n",
        "            self.__ReLu_derivative(x)\n",
        "\n",
        "    def __sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # derivative of sigmoid function, indicates confidence about existing weight\n",
        "\n",
        "    def __sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "    def __tanh(self,x):\n",
        "        return (2*(self.__sigmoid(2*x))-1)\n",
        "        \n",
        "    def __tanh_derivative(self, x):\n",
        "        return (2*self.__sigmoid_derivative(2*x))\n",
        "        \n",
        "    def __Relu(self,x):\n",
        "        x[x <= 0] = 0\n",
        "        return x\n",
        "\n",
        "    def __ReLu_derivative(self,x):\n",
        "        x[x <= 0] = 0\n",
        "        x[x > 0] = 1\n",
        "        return x\n",
        "    #\n",
        "    #  Write code for pre-processing the dataset, which would include standardization, normalization,\n",
        "    #   categorical to numerical, etc\n",
        "    #\n",
        "\n",
        "\n",
        "    def preprocess(self, X):\n",
        "        # replacing the ? values with numpy NaN\n",
        "        X = X.replace('[?]', np.nan, regex=True)\n",
        "        # filling the NaN values by some dummy values\n",
        "        X = X.fillna('0xx0')\n",
        "        ncols = len(X.columns)\n",
        "        nrows = len(X.index)\n",
        "        \n",
        "        # data set encoding\n",
        "        for col in range(ncols):\n",
        "            X[col] = X[col].astype('category')\n",
        "            X[col] = X[col].cat.codes\n",
        "            \n",
        "        scaler = StandardScaler()\n",
        "        # standardization of data set\n",
        "        X=scaler.fit_transform(X)\n",
        "        # returning the dataframe for X\n",
        "        X=pd.DataFrame(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    # Below is the training function\n",
        "\n",
        "    def train(self,activation, max_iterations = 1000, learning_rate = 0.1):\n",
        "        for iteration in range(max_iterations):\n",
        "            out = self.forward_pass(activation)\n",
        "            error = 0.5 * np.power((out - self.y_train), 2)\n",
        "            self.backward_pass(out, activation)\n",
        "            update_layer2 = learning_rate * self.X23.T.dot(self.deltaOut)\n",
        "            update_layer1 = learning_rate * self.X12.T.dot(self.delta23)\n",
        "            update_input = learning_rate * self.X01.T.dot(self.delta12)\n",
        "\n",
        "            self.w23 += update_layer2\n",
        "            self.w12 += update_layer1\n",
        "            self.w01 += update_input\n",
        "\n",
        "        print(\"After \" + str(max_iterations) + \" iterations, the total error is \" + str(np.sum(error)))\n",
        "        print(\"The final weight vectors are (starting from input to output layers)\")\n",
        "        print(self.w01)\n",
        "        print(self.w12)\n",
        "        print(self.w23)\n",
        "\n",
        "    def forward_pass(self,activation):\n",
        "        # pass our inputs through our neural network\n",
        "        in1 = np.dot(self.X_train, self.w01)\n",
        "        self.X12 = self.__activation(in1,activation)\n",
        "        in2 = np.dot(self.X12, self.w12)\n",
        "        self.X23 = self.__activation(in2, activation)\n",
        "        in3 = np.dot(self.X23, self.w23)\n",
        "        out = self.__activation(in3,activation)\n",
        "        return out\n",
        "\n",
        "    # for calculating output for test data set\n",
        "    def testForwardPass(self,activation):\n",
        "        # pass our inputs through our neural network\n",
        "        in1 = np.dot(self.X_test, self.w01)\n",
        "        self.X12 = self.__activation(in1,activation)\n",
        "        in2 = np.dot(self.X12, self.w12)\n",
        "        self.X23 = self.__activation(in2,activation)\n",
        "        in3 = np.dot(self.X23, self.w23)\n",
        "        out = self.__activation(in3,activation)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "    def backward_pass(self, out, activation):\n",
        "        # pass our inputs through our neural network\n",
        "        self.compute_output_delta(out, activation)\n",
        "        self.compute_hidden_layer2_delta(activation)\n",
        "        self.compute_hidden_layer1_delta(activation)\n",
        "\n",
        "    # TODO: Implement other activation functions\n",
        "\n",
        "    def compute_output_delta(self, out, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            delta_output = (self.y_train - out) * (self.__sigmoid_derivative(out))\n",
        "        elif activation == \"tanh\":\n",
        "            delta_output = (self.y_train - out) * (self.__tanh_derivative(out))\n",
        "        else:\n",
        "            delta_output = (self.y_train - out) * (self.__Relu(out))\n",
        "\n",
        "        self.deltaOut = delta_output\n",
        "\n",
        "    # TODO: Implement other activation functions\n",
        "\n",
        "    def compute_hidden_layer2_delta(self, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            delta_hidden_layer2 = (self.deltaOut.dot(self.w23.T)) * (self.__sigmoid_derivative(self.X23))\n",
        "        elif activation == \"tanh\":\n",
        "            delta_hidden_layer2 = (self.deltaOut.dot(self.w23.T)) * (self.__tanh_derivative(self.X23))\n",
        "        else:\n",
        "            delta_hidden_layer2 = (self.deltaOut.dot(self.w23.T)) * (self.__ReLu_derivative(self.X23))\n",
        "\n",
        "        self.delta23 = delta_hidden_layer2\n",
        "\n",
        "    # TODO: Implement other activation functions\n",
        "\n",
        "    def compute_hidden_layer1_delta(self, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            delta_hidden_layer1 = (self.delta23.dot(self.w12.T)) * (self.__sigmoid_derivative(self.X12))\n",
        "        elif activation == \"tanh\":\n",
        "            delta_hidden_layer1 = (self.delta23.dot(self.w12.T)) * (self.__tanh_derivative(self.X12))\n",
        "        else:\n",
        "            delta_hidden_layer1 = (self.delta23.dot(self.w12.T)) * (self.__ReLu_derivative(self.X12))\n",
        "\n",
        "        self.delta12 = delta_hidden_layer1\n",
        "\n",
        "    # TODO: Implement other activation functions\n",
        "\n",
        "    def compute_input_layer_delta(self, activation):\n",
        "        if activation == \"sigmoid\":\n",
        "            delta_input_layer = np.multiply(self.__sigmoid_derivative(self.X01), self.delta01.dot(self.w01.T))\n",
        "        elif activation == \"tanh\":\n",
        "            delta_input_layer = np.multiply(self.__tanh_derivative(self.X01), self.delta01.dot(self.w01.T))\n",
        "        else:\n",
        "            delta_input_layer = np.multiply(self.__ReLu_derivative(self.X01), self.delta01.dot(self.w01.T))\n",
        "\n",
        "\n",
        "            self.delta01 = delta_input_layer\n",
        "\n",
        "    # TODO: Implement the predict function for applying the trained model on the  test dataset.\n",
        "    # You can assume that the test dataset has the same format as the training dataset\n",
        "    # You have to output the test error from this function\n",
        "\n",
        "    # predict function for test data set\n",
        "    def predict(self,activation):\n",
        "        out = self.testForwardPass(activation);\n",
        "        error = 0.5 * np.power((out - self.y_test), 2)\n",
        "        return np.sum(error)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # plug in the URL of data set in parameter of NeuralNet function\n",
        "    neural_network = NeuralNet(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
        "    #Dataset URL'S\n",
        "    #https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "    #https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "    #https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
        "    # activation function implemented\n",
        "    # 1. tanh\n",
        "    # 2. sigmoid\n",
        "    # 3. ReLu\n",
        "    \n",
        "    #Plug in the activation function in parameter of train function.\n",
        "    neural_network.train(\"sigmoid\")\n",
        "    testError = neural_network.predict(\"sigmoid\")\n",
        "    print(\"Test Error: \"+ str(testError))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 1000 iterations, the total error is 39.06004289477169\n",
            "The final weight vectors are (starting from input to output layers)\n",
            "[[-2.32839342  0.34854218  0.46360673  1.12103261]\n",
            " [ 0.84028522  0.22355759 -0.85352422 -0.15839862]\n",
            " [ 3.08555386 -1.81236435  1.49154536 -1.6478605 ]\n",
            " [ 2.41118277 -0.89845492  0.60004566 -2.12984024]]\n",
            "[[-6.07502845 -5.42959156]\n",
            " [ 5.0155346   2.17705345]\n",
            " [-1.99181053 -3.03800078]\n",
            " [ 7.07171862  1.1664247 ]]\n",
            "[[-6.89357088]\n",
            " [-2.73780885]]\n",
            "Test Error: 12.577219998339707\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}